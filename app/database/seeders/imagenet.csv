paper,url,year,model,top1_error,top5_error,flops,multiadds,#params,training_time,time_sec,#epochs,#training_data,gpu,#gpu,tpu,#tpu,hw_gflops,comments
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet50,24.7,7.8,,,,,,,,,,,,,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet101,23.6,7.1,,,,,,,,,,,,,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet152,23,6.7,,,,,,,,,,,,,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNeXt50,22.2,,,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNeXt101,21.2,5.6,,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,VGG16,,,,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,BNInception,25.2,7.82,,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,InceptionResNetv2,19.9,4.9,,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet50 (reimplementation),24.8,7.48,3.86E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet101 (reimplementation),23.17,6.52,7.58E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet152 (reimplementation),22.42,6.34,1.13E+10,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNeXt50 (reimplementation),22.11,5.9,4.24E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNeXt101 (reimplementation),21.18,5.57,7.99E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,VGG16 (reimplementation),27.02,8.81,1.55E+10,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,BNInception (reimplementation),25.38,7.89,2.03E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,InceptionResNetv2 (reimplementation),20.37,5.21,1.18E+10,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet50 (SENet),23.29,6.62,3.87E+09,,2.75E+07,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet101 (SENet),22.38,06.07,7.60E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNet152 (SENet),21.57,5.73,1.13E+10,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNeXt50 (SENet),21.1,5.49,4.25E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ResNeXt101 (SENet),20.7,05.01,8.00E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,VGG16 (SENet),25.22,7.7,1.55E+10,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,BNInception (SENet),24.23,7.14,2.04E+09,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,InceptionResNetv2 (SENet),19.8,4.79,1.18E+10,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,MobileNet,29.4,,,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ShuffleNet,32.6,,,,,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,MobileNet (reimplementation),28.4,9.4,5.69E+08,,4.20E+06,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ShuffleNet (reimplementation),32.6,12.5,1.40E+08,,1.80E+06,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,MobileNet (SENet),25.3,7.7,5.72E+08,,4.70E+06,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
SqueezeandExcitation Networks,https://arxiv.org/abs/1709.01507,2017,ShuffleNet (SENet),31,11.1,1.42E+08,,2.40E+06,,,100,1200000,GeForce Titan X,8,,,8.78E+04,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,IncepResNet V2,19.6,4.7,,,1.32E+10,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,ResNeXt101,19.1,4.4,,,3.15E+10,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,PolyNet,18.7,4.2,,,3.47E+10,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,DualPathNet131,18.5,4.2,,,3.20E+10,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,GeNet2,27.9,9.6,,,,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,BlockQNNB,24.3,7.4,,,,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,Hierarchical,20.3,5.2,,,,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,NASNetA,17.3,3.8,,,2.38E+10,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,PNASNet5,17.1,3.8,,,2.50E+10,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,"AmoebaNetA (N=6, F=190)",17.2,3.9,,,2.31E+10,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
Regularized Evolution for Image Classifier Architecture Search,https://arxiv.org/abs/1802.01548,2018,"AmoebaNetA (N=6, F=448)",16.1,3.4,,,1.04E+11,,,,1200000,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
,https://github.com/albanie/convnetburden,2018,caffenet,42.6,19.7,7.24E+08,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,squeezenet10,41.9,19.58,8.37E+08,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,squeezenet11,41.81,19.38,3.60E+08,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,vggf,41.4,19.1,7.27E+08,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,vggm128,40.8,18.4,2.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,vggm1024,37.8,16.1,2.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,vggm2048,37.1,15.8,2.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,vggs,37,15.8,3.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,vggm,36.9,15.5,2.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,googlenet,34.2,12.9,2.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,resnet18,30.24,10.92,2.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,mcnmobilenet,29.4,,5.79E+08,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,vggvd19,28.7,9.9,2.00E+10,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,vggvd16,28.5,9.9,1.60E+10,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,resnet34,26.7,8.58,4.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,densenet121,25.35,7.83,3.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,resnet50,24.6,7.7,4.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,densenet169,24,7,3.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,SEBNInception,23.62,07.04,2.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,resnet101,23.4,7,8.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,resnet152,23,6.7,1.10E+10,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,densenet201,22.8,6.43,4.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,resnext5032x4d,22.6,6.49,4.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,inceptionv3,22.55,6.44,6.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,SEResNet50,22.37,6.36,4.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,densenet161,22.35,6.2,8.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,SEResNet101,21.75,5.72,8.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,resnext10132x4d,21.55,5.93,8.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,SEResNet152,21.34,5.54,1.10E+10,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,SEResNeXt5032x4d,20.97,5.54,4.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,resnext10164x4d,20.81,5.66,1.60E+10,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,SEResNeXt10132x4d,19.81,4.96,8.00E+09,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
,https://github.com/albanie/convnetburden,2018,SENet,18.68,4.47,2.10E+10,,,,,,1200000,,,,,,parameter in memory / The numbers are given for single element batches.
AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,2019,ShuffleNet2x,30.2,,,,,,,,1200000,,,,,,
AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,2019,MobileNetv1,29.4,,,,,,,,1200000,,,,,,
AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,2019,NASNetC,27.5,,,,,,,,1200000,,,,,,
AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,2019,NASNetB,27.2,,,,,,,,1200000,,,,,,
AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,2019,DARTS,26.7,8.7,,5.74E+08,4.70E+06,,,,1200000,GeForce GTX 1080 Ti,10,,,1.13E+05,"on ImageNet, not trained in this paper"
AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/pdf/1808.00193.pdf,2019,RESNASNet,24.3,7.4,,5.80E+08,5.36E+06,,,200,1200000,,,,,,
AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search,https://arxiv.org/abs/1903.11059,2019,AlphaX1,24.5,7.8,,5.79E+08,5.40E+06,8 GPU days,6.91E+04,200,1200000,,,,,,on ImageNet
Aggregated Residual Transformations for Deep Neural Networks,https://arxiv.org/abs/1611.05431,2016,ResNeXt101 (64 x 4d) img size 320x320 / 299x299,19.1,4.4,,,,10 GPU days,1.08E+05,94,1200000,Tesla M40,8,,,5.48E+04,Training the 2×complexity model (64×4d ResNeXt101) takes 1.7s per minibatch and 10 days total on 8 GPUs
Aggregated Residual Transformations for Deep Neural Networks,https://arxiv.org/abs/1611.05431,2016,ResNeXt101 (64 x 4d) img size 224 x 224,20.4,5.3,,,,10 GPU days,1.08E+05,94,1200000,Tesla M40,8,,,5.48E+04,Training the 2×complexity model (64×4d ResNeXt101) takes 1.7s per minibatch and 10 days total on 8 GPUs
Aggregated Residual Transformations for Deep Neural Networks,https://arxiv.org/abs/1611.05431,2016,ResNeXt101 (32 x 4d) 1Kway classification,22.2,5.7,7.80E+09,,4.40E+07,10 GPU days,1.08E+05,94,1200000,Tesla M40,8,,,5.48E+04,"On 8 GPUs of NVIDIA M40, training 32×4d ResNeXt101 in Table 3 takes 0.95s per minibatch,"
Aggregated Residual Transformations for Deep Neural Networks,https://arxiv.org/abs/1611.05431,2016,ResNeXt50 (32 x 4d) 1Kway classification,24.4,6.6,4.20E+09,,2.50E+07,10 GPU days,1.08E+05,94,1200000,Tesla M40,8,,,5.48E+04,
Fig 2 of IDK Cascades: Fast Deep Learning by Learning not to Overthink,https://arxiv.org/abs/1706.00885,2017,AlexNet,43.4,,7.00E+08,7.00E+08,,,,,1200000,,,,,,
Fig 2 of IDK Cascades: Fast Deep Learning by Learning not to Overthink,https://arxiv.org/abs/1706.00885,2017,ResNet18,30.2,,1.80E+09,1.80E+09,,,,,1200000,,,,,,
Fig 2 of IDK Cascades: Fast Deep Learning by Learning not to Overthink,https://arxiv.org/abs/1706.00885,2017,ResNet34,26.7,,3.60E+09,3.60E+09,,,,,1200000,,,,,,
Fig 2 of IDK Cascades: Fast Deep Learning by Learning not to Overthink,https://arxiv.org/abs/1706.00885,2017,ResNet50,23.8,,3.80E+09,3.80E+09,,,,,1200000,,,,,,
Fig 2 of IDK Cascades: Fast Deep Learning by Learning not to Overthink,https://arxiv.org/abs/1706.00885,2017,ResNet101,22.6,,7.60E+09,7.60E+09,,,,,1200000,,,,,,
Fig 2 of IDK Cascades: Fast Deep Learning by Learning not to Overthink,https://arxiv.org/abs/1706.00885,2017,ResNet152,21.7,,1.13E+10,1.13E+10,,,,,1200000,,,,,,
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,NASNetA (6@4032),17.3,3.8,,2.38E+10,8.89E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,NASNetA (7 @1920),19.2,4.7,,4.93E+09,2.26E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,NASNetA (5 @ 1538),21.4,5.8,,2.35E+09,1.09E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,NASNetC (3 @960),27.5,9,,5.58E+08,4.90E+06,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,NASNetB(4 @1536),27.2,8.7,,4.88E+08,5.30E+06,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,NASNetA (4 @1056),26,8.4,,5.64E+08,5.30E+06,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,Inceptionv1,30.2,10.1,,1.45E+09,6.60E+06,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,MobileNet224,29.4,10.5,,5.69E+08,4.20E+06,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,ShuffleNet (2x),29.1,10.2,,5.24E+08,5.00E+06,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,Inceptionv2,25.2,7.8,,1.94E+09,1.12E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,Inceptionv3,21.2,5.6,,5.72E+09,2.38E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,Xception,21,5.5,,8.38E+09,2.28E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,InceptionResNetv2,19.9,4.9,,1.32E+10,5.58E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,ResNeXt101 (64 x 4d),19.1,4.4,,3.15E+10,8.36E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,PolyNet,18.7,4.2,,3.47E+10,9.20E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,DPN131,18.5,4.2,,3.20E+10,7.95E+07,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,SENet,17.3,3.8,,4.23E+10,1.46E+08,96h,3.46E+05,20,50000,Tesla P100 PCIe 16 GB,500,,,4.76E+06,this paper trainnig was done with CIFAR
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,"NASNetA (N=6, F=168)",17.3,,,,,,,,,,,,,,
Learning Transferable Architectures for Scalable Image Recognition,https://arxiv.org/abs/1707.07012,2018,NASNetA,17.3,,,,,,,,,,,,,,
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,MobileNet2241.0,29.4,10.5,,5.69E+08,,,,,50000,,,,,,not trained in this paper
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,ShuffleNet 2x,29.1,10.2,,5.24E+08,,,,,50000,,,,,,not trained in this paper
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,CondenseNet (g1=g3=8),29,10,,2.74E+08,,,,,50000,,,,,,not trained in this paper
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,NASNetC (N=3),27.5,9,,5.58E+08,,,,,50000,,,,,,not trained in this paper
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,NASNetB (N=4),27.2,8.7,,4.48E+08,,,,,50000,,,,,,not trained in this paper
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,CondenseNet (g1=g3=4),26.2,8.3,,5.29E+08,,,,,50000,,,,,,not trained in this paper
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,NASNetA (N=4),26,8.4,,5.64E+08,,,,,50000,,,,,,not trained in this paper
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,"TreeCellA with CondenseNet(g1=4, g3=8)",25.5,8,,5.88E+08,,200 GPUhours,,300,50000,,,,,,trained on CIFAR10
Pathlevel Network Transformation for Efficient Architecture Search,https://arxiv.org/abs/1806.02639,2018,"TreeCellB with CondenseNet(g1=4, g3=8)",25.4,8.1,,5.94E+08,,200 GPUhours,,300,50000,,,,,,trained on CIFAR10
PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,2017,REsNet152,20.93,5.54,,,,,,,1200000,GeForce Titan X,8,,,8.78E+04,All training procedures were terminated after 560K iterations
PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,2017,ResNet269,19.78,4.89,,,,,,,1200000,GeForce Titan X,8,,,8.78E+04,All training procedures were terminated after 560K iterations
PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,2017,ResNet500,19.66,4.78,,,,,,,1200000,GeForce Titan X,8,,,8.78E+04,All training procedures were terminated after 560K iterations
PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,2017,InceptrionResNetV2,20.05,05.05,,,,,,,1200000,GeForce Titan X,8,,,8.78E+04,All training procedures were terminated after 560K iterations
PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,2017,Very Deep InceptionResNet,19.1,4.48,,,,,,,1200000,GeForce Titan X,8,,,8.78E+04,All training procedures were terminated after 560K iterations
PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,https://arxiv.org/abs/1611.05725,2017,Very Deep Polynet,18.71,4.25,,,,,,,1200000,GeForce Titan X,8,,,8.78E+04,All training procedures were terminated after 560K iterations
"Inceptionv4, InceptionResNet and the Impact of Residual Connections on Learning",https://arxiv.org/abs/1602.07261,2016,Inception V4,20,5,,,,,,160,1200000,,20,,,,
"Inceptionv4, InceptionResNet and the Impact of Residual Connections on Learning",https://arxiv.org/abs/1602.07261,2016,IncepResNet V1,21.3,5.5,,,,,,200,1200000,,20,,,,
"Inceptionv4, InceptionResNet and the Impact of Residual Connections on Learning",https://arxiv.org/abs/1602.07261,2016,IncepResNet V2,19.9,4.9,,,,,,160,1200000,,20,,,,
Progressive Neural Architecture Search,https://arxiv.org/abs/1712.00559,2018,"PNASNet5 (N = 3, F = 54)",25.8,8.1,,6.00E+08,5.10E+06,,,,,Tesla P100 PCIe 16 GB,50,,,4.76E+05,
Progressive Neural Architecture Search,https://arxiv.org/abs/1712.00559,2018,"PNASNet5 (N=4, F=216)",17.1,3.8,,2.50E+10,8.61E+07,,,,,Tesla P100 PCIe 16 GB,100,,,9.53E+05,
PruneTrain: Gradual Structured Pruning from Scratch for Faster Neural Network Training,https://arxiv.org/abs/1901.09290,2019,ResNet50,25.3,,,,,,,,,,,,,,
Dual Path Networks,https://arxiv.org/abs/1707.01629,2017,"DPN92 (32 x 3d), size = 145 MB img size 224 x 224",20.7,5.4,6.50E+09,,3.78E+07,,,,1200000,Tesla K80,4,,,2.24E+04,Not shown on the graph
Dual Path Networks,https://arxiv.org/abs/1707.01629,2017,"DPN92 (32 x 3d), size = 145 MB img size  x320",19.3,4.7,6.50E+09,,3.78E+07,,,,1200000,Tesla K80,4,,,2.24E+04,Not shown on the graph
Dual Path Networks,https://arxiv.org/abs/1707.01629,2017,"DPN98 (40 x 4d), size = 236 MB img size 224 x 224",20.2,5.2,1.17E+10,,6.17E+07,86 samples per second /batch =24,,,1200000,Tesla K80,4,,,2.24E+04,Fig 3
Dual Path Networks,https://arxiv.org/abs/1707.01629,2017,"DPN98 (40 x 4d), size = 236 MB img size 320 x 320",18.9,4.4,1.17E+10,,6.17E+07,86 samples per second /batch =24,,,1200000,Tesla K80,4,,,2.24E+04,Fig 3
Dual Path Networks,https://arxiv.org/abs/1707.01629,2017,"DPN131 (40 × 4d), size = 304 MB img size 224 x 224",19.93,5.12,1.60E+10,,7.95E+07,62 samples per second/batch =24,,,1200000,Tesla K80,4,,,2.24E+04,Fig 3
Dual Path Networks,https://arxiv.org/abs/1707.01629,2017,"DPN131 (40 × 4d), size = 304 MB img size 320 x 320",18.62,4.23,1.60E+10,,7.95E+07,62 samples per second/batch =24,,,1200000,Tesla K80,4,,,2.24E+04,Fig 3
Dual Path Networks,https://arxiv.org/abs/1707.01629,2017,"DPN131 (40 × 4d), size = 304 MB img size 224 x 224 / Mean Max Pooling",19.93,5.12,1.60E+10,,7.95E+07,62 samples per second/batch =24,,,1200000,Tesla K80,4,,,2.24E+04,Fig 3
Dual Path Networks,https://arxiv.org/abs/1707.01629,2017,"DPN131 (40 × 4d), size = 304 MB img size 320 x 320 / Mean Max Pooling",18.55,4.16,1.60E+10,100,7.95E+07,62 samples per second/batch =24,,,1200000,Tesla K80,4,,,2.24E+04,Fig 3
Selftraining with Noisy Student improves ImageNet classification,https://arxiv.org/abs/1911.04252,2020,NoisyStudent(EfficientNetL2),11.6,1.3,,,4.80E+08,6 days,5.18E+05,700,1200000,,,Cloud TPU v3 Pod,1,1.00E+08,
Selftraining with Noisy Student improves ImageNet classification,https://arxiv.org/abs/1911.04252,2020,NoisyStudent(EfficientNetL2),12.6,,,,,6 days,5.18E+05,700,1200000,,,Cloud TPU v3 Pod,1,1.00E+08,
Fixing the traintest resolution discrepancy,https://arxiv.org/pdf/1906.06423v3.pdf,2019,FixResNeXt101 32x48d,13.6,2,,,8.29E+08,151.5 hours,5.46E+05,,1200000,Tesla V100 PCIe 32 GB,8,,,1.13E+05,
Fixing the traintest resolution discrepancy,https://arxiv.org/pdf/1906.06423v3.pdf,2019,FixResNet50 Billion,17.5,3.9,,,2.56E+07,151.5 hours,5.46E+05,,1200000,Tesla V100 PCIe 32 GB,8,,,1.13E+05,
Fixing the traintest resolution discrepancy,https://arxiv.org/pdf/1906.06423v3.pdf,2019,FixResNet50 CutMix,20.2,5.1,,,2.56E+07,151.5 hours,5.46E+05,,1200000,Tesla V100 PCIe 32 GB,8,,,1.13E+05,
Fixing the traintest resolution discrepancy,https://arxiv.org/pdf/1906.06423v3.pdf,2019,FixResNet50,20.9,5.4,,,2.56E+07,151.5 hours,5.46E+05,,1200000,Tesla V100 PCIe 32 GB,8,,,1.13E+05,
SCARLETNAS: Bridging the Gap between Stability and Scalability in Weightsharing Neural Architecture Search,https://arxiv.org/pdf/1908.06022v5.pdf,2020,SCARLETA,23.1,6.6,,3.65E+08,6.70E+06,10 GPU days,4.32E+05,,1200000,Tesla V100 PCIe 32 GB,2,,,2.83E+04,
SCARLETNAS: Bridging the Gap between Stability and Scalability in Weightsharing Neural Architecture Search,https://arxiv.org/pdf/1908.06022v5.pdf,2020,SCARLETA4,17.7,4,4.20E+09,1.00E+09,2.78E+07,10 GPU days,8.64E+05,,1200000,Tesla V100 PCIe 32 GB,1,,,1.41E+04,
SCARLETNAS: Bridging the Gap between Stability and Scalability in Weightsharing Neural Architecture Search,https://arxiv.org/pdf/1908.06022v5.pdf,2020,SCARLETC,24.4,7.4,,2.80E+08,6.00E+06,10 GPU days,8.64E+05,,1200000,Tesla V100 PCIe 32 GB,1,,,1.41E+04,
SCARLETNAS: Bridging the Gap between Stability and Scalability in Weightsharing Neural Architecture Search,https://arxiv.org/pdf/1908.06022v5.pdf,2020,SCARLETB,23.7,7,3.29E+08,3.29E+08,6.50E+06,10 GPU days,8.64E+05,,1200000,Tesla V100 PCIe 32 GB,1,,,1.41E+04,
SCARLETNAS: Bridging the Gap between Stability and Scalability in Weightsharing Neural Architecture Search,https://arxiv.org/pdf/1908.06022v5.pdf,2020,SCARLETA2,20.5,5.2,,1.00E+09,1.25E+07,10 GPU days,8.64E+05,,1200000,Tesla V100 PCIe 32 GB,1,,,1.41E+04,
MnasNet: PlatformAware Neural Architecture Search for Mobile,https://arxiv.org/pdf/1807.11626v3.pdf,2019,MnasNetA1,24.8,7.5,,3.12E+08,3.90E+06,4.5 days,3.89E+05,5,50000,,,Cloud TPU v2,64,1.80E+05,
MnasNet: PlatformAware Neural Architecture Search for Mobile,https://arxiv.org/pdf/1807.11626v3.pdf,2019,MnasNetA2,24.6,7.3,,3.40E+08,4.80E+06,4.5 days,3.89E+05,5,50000,,,Cloud TPU v3,64,1.80E+05,
MnasNet: PlatformAware Neural Architecture Search for Mobile,https://arxiv.org/pdf/1807.11626v3.pdf,2019,MnasNetA3,23.3,6.7,,4.03E+08,5.20E+06,4.5 days,3.89E+05,5,50000,,,Cloud TPU v4,64,1.80E+05,
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGESCALE IMAGE RECOGNITION,https://arxiv.org/pdf/1409.1556v6.pdf,2015,VGG19,25.5,8,,,,2~3 weeks,1.51E+06,74,1200000,GeForce GTX Titan Black,4,,,2.05E+04,
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGESCALE IMAGE RECOGNITION,https://arxiv.org/pdf/1409.1556v6.pdf,2015,VGG16,25.6,8.1,,,,2~3 weeks,1.51E+06,74,1200000,GeForce GTX Titan Black,4,,,2.05E+04,
Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,https://arxiv.org/pdf/1406.4729v4.pdf,2015,SPPNet,27.86,,,,,2 ~ 4 weeks,1.81E+06,90,1200000,,1,,,,
Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,https://arxiv.org/pdf/1406.4729v4.pdf,2015,MSRA,28.68,,,,,2 ~ 4 weeks,1.81E+06,90,1200000,,1,,,,
Visualizing and Understanding Convolutional Networks,https://arxiv.org/pdf/1311.2901v3.pdf,2013,"ZFNet(ensemble, 6 convnets)",36,14.7,,,,12 days,1.04E+06,70,1200000,GeForce GTX 580,1,,,1.58E+03,
Visualizing and Understanding Convolutional Networks,https://arxiv.org/pdf/1311.2901v3.pdf,2013,"ZFNet(1 convnet, 512,1024,512 maps)",37.5,16,,,,12 days,1.04E+06,70,1200000,GeForce GTX 580,1,,,1.58E+03,
Xception: Deep Learning with Depthwise Separable Convolutions,https://arxiv.org/pdf/1610.02357v3.pdf,2017,Xception,21,5.5,,,2.29E+07,3 days,2.59E+05,,1200000,Tesla K80,60,,,3.36E+05,
Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution,https://arxiv.org/pdf/1904.05049v3.pdf,2019,OctResNet152(SE) 224x224,17.1,3.7,2.22E+10,,6.68E+07,95 imgs/sec,1.47E+09,,1200000,Tesla V100 PCIe 32 GB,1,,,1.41E+04,
Searching Beyond MobileNetV3,https://arxiv.org/pdf/1908.01314v4.pdf,2020,MoGAA,24.1,7.2,,3.04E+08,5.10E+06,12 GPU days,,,1200000,,,,,,
Delving Deep into Rectifiers: Surpassing HumanLevel Performance on ImageNet Classification,https://arxiv.org/pdf/1502.01852v1.pdf,2015,PReLUNet,24.27,7.38,,,,3 ~ 4 weeks,2.12E+06,80,1200000,Tesla K20,4,,,1.41E+04,
ImageNet Classification with Deep Convolutional Neural Networks,http://papers.nips.cc/paper/4824imagenetclassificationwithdeepconvolutionalneuralnetworks.pdf,2012,AlexNet  7CNNs,36.7,15.3,,,6.00E+07,5 ~ 6 days,4.75E+05,90,1200000,GeForce GTX 580,2,,,3.16E+03,